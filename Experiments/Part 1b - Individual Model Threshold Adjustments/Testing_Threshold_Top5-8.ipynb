{"cells":[{"cell_type":"markdown","metadata":{"id":"CKeVGxZ5GG6o"},"source":["# Import needed modules"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GBYLqPKdUB5s","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1692002622259,"user_tz":-480,"elapsed":3256,"user":{"displayName":"Thesis TempF","userId":"15864590997921893499"}},"outputId":"5b79e5de-f8b1-4476-9307-e4024db13419"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["model1_name = 'EfficientNetV2B3'\n","model2_name = 'ResNet50V2'\n","model3_name = 'InceptionResNetV2'\n","model4_name = 'VGG16'\n","\n","from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CeMcAy_5GG6s","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1692002634178,"user_tz":-480,"elapsed":11924,"user":{"displayName":"Thesis TempF","userId":"15864590997921893499"}},"outputId":"8db7a122-114c-4ee8-a3d6-e2ae45124a97"},"outputs":[{"output_type":"stream","name":"stdout","text":["modules loaded\n"]}],"source":["# import system libs\n","import os\n","import time\n","import shutil\n","import pathlib\n","import itertools\n","\n","# import data handling tools\n","import cv2\n","import numpy as np\n","import pandas as pd\n","import seaborn as sns\n","sns.set_style('darkgrid')\n","import matplotlib.pyplot as plt\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import confusion_matrix, classification_report\n","from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n","\n","\n","# import Deep learning Libraries\n","import tensorflow as tf\n","from tensorflow import keras\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.optimizers import Adam, Adamax\n","from tensorflow.keras.metrics import categorical_crossentropy\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Activation, Dropout, BatchNormalization\n","from tensorflow.keras import regularizers\n","\n","# Ignore Warnings\n","import warnings\n","warnings.filterwarnings(\"ignore\")\n","\n","print ('modules loaded')"]},{"cell_type":"markdown","metadata":{"id":"SA_gwvwnGG6v"},"source":["# Create needed functions"]},{"cell_type":"markdown","metadata":{"id":"JQdhl_CRGG6v"},"source":["#### **Function to create data frame**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"g2nDmYaAabWE"},"outputs":[],"source":["# Generate data paths with labels\n","def define_paths(dir):\n","    filepaths = []\n","    labels = []\n","\n","    folds = os.listdir(dir)\n","    for fold in folds:\n","        foldpath = os.path.join(dir, fold)\n","        filelist = os.listdir(foldpath)\n","\n","        for fold_ in filelist:\n","            foldpath_ = os.path.join(foldpath, fold_)\n","            filelist_ = os.listdir(foldpath_)\n","\n","            for file_ in filelist_:\n","                fpath = os.path.join(foldpath_, file_)\n","                filepaths.append(fpath)\n","                labels.append(fold_)\n","\n","    return filepaths, labels\n","\n","\n","# Concatenate data paths with labels into one dataframe ( to later be fitted into the model )\n","def define_df(files, classes):\n","    Fseries = pd.Series(files, name= 'filepaths')\n","    Lseries = pd.Series(classes, name='labels')\n","    return pd.concat([Fseries, Lseries], axis= 1)\n","\n","\n","# Function that create dataframe for train, validation, and test data\n","def create_df(data_dir):\n","\n","    # train dataframe\n","    files, classes = define_paths(data_dir[0])\n","    df = define_df(files, classes)\n","\n","    strat = df['labels']\n","    train_df, valid_df = train_test_split(df, train_size=0.85, shuffle=True, random_state=123, stratify=strat)\n","\n","    # test dataframe\n","    files, classes = define_paths(data_dir[1])\n","    test_df = define_df(files, classes)\n","\n","    return train_df, valid_df, test_df"]},{"cell_type":"markdown","metadata":{"id":"JZaHdeFxGG6x"},"source":["#### Function to generate images from dataframe"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iLL8hHQcGG6x"},"outputs":[],"source":["def create_model_data (train_df, valid_df, test_df, batch_size):\n","    '''\n","    This function takes train, validation, and test dataframe and fit them into image data generator, because model takes data from image data generator.\n","    Image data generator converts images into tensors. '''\n","\n","\n","    # define model parameters\n","    img_size = (224, 224)\n","    channels = 3 # either BGR or Grayscale\n","    color = 'rgb'\n","    img_shape = (img_size[0], img_size[1], channels)\n","\n","    # Recommended : use custom function for test data batch size, else we can use normal batch size.\n","    ts_length = len(test_df)\n","    test_batch_size = max(sorted([ts_length // n for n in range(1, ts_length + 1) if ts_length%n == 0 and ts_length/n <= 80]))\n","    test_steps = ts_length // test_batch_size\n","\n","    # This function which will be used in image data generator for data augmentation, it just take the image and return it again.\n","    def scalar(img):\n","        return img\n","\n","    tr_gen = ImageDataGenerator(\n","      preprocessing_function= scalar,\n","      rotation_range=50,\n","      horizontal_flip=True,\n","      vertical_flip=True\n","    )\n","\n","    ts_gen = ImageDataGenerator(preprocessing_function= scalar)\n","\n","    train_gen = tr_gen.flow_from_dataframe( train_df, x_col= 'filepaths', y_col= 'labels', target_size= img_size, class_mode= 'categorical',\n","                                        color_mode= color, shuffle= True, batch_size= batch_size)\n","\n","    valid_gen = ts_gen.flow_from_dataframe( valid_df, x_col= 'filepaths', y_col= 'labels', target_size= img_size, class_mode= 'categorical',\n","                                        color_mode= color, shuffle= True, batch_size= batch_size)\n","\n","    # Note: we will use custom test_batch_size, and make shuffle= false\n","    test_gen = ts_gen.flow_from_dataframe( test_df, x_col= 'filepaths', y_col= 'labels', target_size= img_size, class_mode= 'categorical',\n","                                        color_mode= color, shuffle= False, batch_size= test_batch_size)\n","\n","    return train_gen, valid_gen, test_gen"]},{"cell_type":"markdown","metadata":{"id":"57eDFl3oGG65"},"source":["# **Model Structure**"]},{"cell_type":"markdown","metadata":{"id":"2GHNMVrhGG65"},"source":["#### **Start Reading Dataset**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FWfxfQEVabWS","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1692002638764,"user_tz":-480,"elapsed":4593,"user":{"displayName":"Thesis TempF","userId":"15864590997921893499"}},"outputId":"82f36ebb-f111-4324-810b-05b7df78389b"},"outputs":[{"output_type":"stream","name":"stdout","text":["Found 6830 validated image filenames belonging to 2 classes.\n","Found 1205 validated image filenames belonging to 2 classes.\n","Found 4522 validated image filenames belonging to 2 classes.\n"]}],"source":["data_dir = ['/content/drive/MyDrive/C-NMC_Leukemia/training_data','/content/drive/MyDrive/C-NMC_Leukemia/validation_data']\n","\n","try:\n","    # Get splitted data\n","    train_df, valid_df, test_df = create_df(data_dir)\n","\n","    # Get Generators\n","    batch_size = 40\n","    train_gen, valid_gen, test_gen = create_model_data(train_df, valid_df, test_df, batch_size)\n","\n","except:\n","    print('Invalid Input')"]},{"cell_type":"markdown","metadata":{"id":"3wvOKjeRGG65"},"source":["#### **Generic Model Creation**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kDT4CV15abWT","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1692002665254,"user_tz":-480,"elapsed":26525,"user":{"displayName":"Thesis TempF","userId":"15864590997921893499"}},"outputId":"c04fbd95-b04d-48e6-c593-25e7708436d3"},"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"sequential\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," efficientnetv2-b3 (Function  (None, 1536)             12930622  \n"," al)                                                             \n","                                                                 \n"," batch_normalization (BatchN  (None, 1536)             6144      \n"," ormalization)                                                   \n","                                                                 \n"," dense (Dense)               (None, 256)               393472    \n","                                                                 \n"," dropout (Dropout)           (None, 256)               0         \n","                                                                 \n"," dense_1 (Dense)             (None, 2)                 514       \n","                                                                 \n","=================================================================\n","Total params: 13,330,752\n","Trainable params: 13,218,464\n","Non-trainable params: 112,288\n","_________________________________________________________________\n","Model: \"sequential_1\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," resnet50v2 (Functional)     (None, 2048)              23564800  \n","                                                                 \n"," batch_normalization_1 (Batc  (None, 2048)             8192      \n"," hNormalization)                                                 \n","                                                                 \n"," dense_2 (Dense)             (None, 256)               524544    \n","                                                                 \n"," dropout_1 (Dropout)         (None, 256)               0         \n","                                                                 \n"," dense_3 (Dense)             (None, 2)                 514       \n","                                                                 \n","=================================================================\n","Total params: 24,098,050\n","Trainable params: 24,048,514\n","Non-trainable params: 49,536\n","_________________________________________________________________\n","Model: \"sequential_2\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," inception_resnet_v2 (Functi  (None, 1536)             54336736  \n"," onal)                                                           \n","                                                                 \n"," batch_normalization_205 (Ba  (None, 1536)             6144      \n"," tchNormalization)                                               \n","                                                                 \n"," dense_4 (Dense)             (None, 256)               393472    \n","                                                                 \n"," dropout_2 (Dropout)         (None, 256)               0         \n","                                                                 \n"," dense_5 (Dense)             (None, 2)                 514       \n","                                                                 \n","=================================================================\n","Total params: 54,736,866\n","Trainable params: 54,673,250\n","Non-trainable params: 63,616\n","_________________________________________________________________\n","Model: \"sequential_3\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," vgg16 (Functional)          (None, 512)               14714688  \n","                                                                 \n"," batch_normalization_206 (Ba  (None, 512)              2048      \n"," tchNormalization)                                               \n","                                                                 \n"," dense_6 (Dense)             (None, 256)               131328    \n","                                                                 \n"," dropout_3 (Dropout)         (None, 256)               0         \n","                                                                 \n"," dense_7 (Dense)             (None, 2)                 514       \n","                                                                 \n","=================================================================\n","Total params: 14,848,578\n","Trainable params: 14,847,554\n","Non-trainable params: 1,024\n","_________________________________________________________________\n"]}],"source":["# Create Model Structure\n","img_size = (224, 224)\n","channels = 3\n","img_shape = (img_size[0], img_size[1], channels)\n","class_count = len(list(train_gen.class_indices.keys())) # to define number of classes in dense layer\n","\n","# MODEL 1\n","base_model = tf.keras.applications.EfficientNetV2B3(include_top= False, weights= \"imagenet\", input_shape= img_shape, pooling= 'max')\n","model1 = Sequential([\n","    base_model,\n","    BatchNormalization(axis= -1, momentum= 0.99, epsilon= 0.001),\n","    Dense(256, kernel_regularizer= regularizers.l2(l= 0.016), activity_regularizer= regularizers.l1(0.006),\n","                bias_regularizer= regularizers.l1(0.006), activation= 'relu'),\n","    Dropout(rate= 0.45, seed= 123),\n","    Dense(class_count, activation= 'softmax')\n","])\n","model1.compile(Adamax(learning_rate= 0.0012), loss= 'categorical_crossentropy', metrics= ['accuracy'])\n","model1.load_weights(f'/content/drive/MyDrive/Models/ALL-final-{model1_name}-weights.h5')\n","model1.summary()\n","\n","# MODEL 2\n","base_model = tf.keras.applications.ResNet50V2(include_top= False, weights= \"imagenet\", input_shape= img_shape, pooling= 'max')\n","model2 = Sequential([\n","    base_model,\n","    BatchNormalization(axis= -1, momentum= 0.99, epsilon= 0.001),\n","    Dense(256, kernel_regularizer= regularizers.l2(l= 0.016), activity_regularizer= regularizers.l1(0.006),\n","                bias_regularizer= regularizers.l1(0.006), activation= 'relu'),\n","    Dropout(rate= 0.45, seed= 123),\n","    Dense(class_count, activation= 'softmax')\n","])\n","model2.compile(Adamax(learning_rate= 0.0012), loss= 'categorical_crossentropy', metrics= ['accuracy'])\n","model2.load_weights(f'/content/drive/MyDrive/Models/ALL-final-{model2_name}-weights.h5')\n","model2.summary()\n","\n","# MODEL 3\n","base_model = tf.keras.applications.InceptionResNetV2(include_top= False, weights= \"imagenet\", input_shape= img_shape, pooling= 'max')\n","model3 = Sequential([\n","    base_model,\n","    BatchNormalization(axis= -1, momentum= 0.99, epsilon= 0.001),\n","    Dense(256, kernel_regularizer= regularizers.l2(l= 0.016), activity_regularizer= regularizers.l1(0.006),\n","                bias_regularizer= regularizers.l1(0.006), activation= 'relu'),\n","    Dropout(rate= 0.45, seed= 123),\n","    Dense(class_count, activation= 'softmax')\n","])\n","model3.compile(Adamax(learning_rate= 0.0012), loss= 'categorical_crossentropy', metrics= ['accuracy'])\n","model3.load_weights(f'/content/drive/MyDrive/Models/ALL-final-{model3_name}-weights.h5')\n","model3.summary()\n","\n","# MODEL 4\n","base_model = tf.keras.applications.VGG16(include_top= False, weights= \"imagenet\", input_shape= img_shape, pooling= 'max')\n","model4 = Sequential([\n","    base_model,\n","    BatchNormalization(axis= -1, momentum= 0.99, epsilon= 0.001),\n","    Dense(256, kernel_regularizer= regularizers.l2(l= 0.016), activity_regularizer= regularizers.l1(0.006),\n","                bias_regularizer= regularizers.l1(0.006), activation= 'relu'),\n","    Dropout(rate= 0.45, seed= 123),\n","    Dense(class_count, activation= 'softmax')\n","])\n","model4.compile(Adamax(learning_rate= 0.0012), loss= 'categorical_crossentropy', metrics= ['accuracy'])\n","model4.load_weights(f'/content/drive/MyDrive/Models/ALL-final-{model4_name}-weights.h5')\n","model4.summary()"]},{"cell_type":"markdown","metadata":{"id":"MySXhfAJGG68"},"source":["# **Evaluate model**"]},{"cell_type":"markdown","metadata":{"id":"4l-DABtFGG68"},"source":["# **Get Predictions**"]},{"cell_type":"code","source":["g_dict = test_gen.class_indices\n","classes = list(g_dict.keys())\n","\n","ts_length = len(test_df)\n","test_batch_size = test_batch_size = max(sorted([ts_length // n for n in range(1, ts_length + 1) if ts_length%n == 0 and ts_length/n <= 80]))\n","test_steps = ts_length // test_batch_size"],"metadata":{"id":"avbE3DBTvNEB"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### **Tasks per fold:**\n","\n","1. Make predictions\n","2. Adjust thresholds and produce classification report for each threshold pair\n","3. Get best threshold levels for each result metric\n","4. Display adjusted confusion matrix and metrics with improvement\n","5. Append top [chosen metric=> Weighted F1-Score] to the scores list"],"metadata":{"id":"Et92Q-YwjHFf"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"O1V-seI9iihJ","colab":{"base_uri":"https://localhost:8080/"},"outputId":"953dd9d9-3148-4197-a2be-9e2ff07193a9","executionInfo":{"status":"ok","timestamp":1692002945076,"user_tz":-480,"elapsed":279842,"user":{"displayName":"Thesis TempF","userId":"15864590997921893499"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["**********************************\n","Model # 1\n","**********************************\n","119/119 [==============================] - 35s 224ms/step - loss: 0.3125 - accuracy: 0.9049\n","--------------------\n","Original threshold\n","Test Loss:  0.31249740719795227\n","Test Accuracy:  0.9049093127250671\n","\n","Confusion matrix:\n","[[2857  184]\n"," [ 246 1235]]\n","              precision    recall  f1-score   support\n","\n","         all     0.9207    0.9395    0.9300      3041\n","         hem     0.8703    0.8339    0.8517      1481\n","\n","    accuracy                         0.9049      4522\n","   macro avg     0.8955    0.8867    0.8909      4522\n","weighted avg     0.9042    0.9049    0.9044      4522\n","\n","\n","For the threshold adjustments on test set predictions for Model 1\n","Best accuracy           - threshold [0.5,0.5] -> 90.49%\n","Best ALL precision      - threshold [0.9,0.1] -> 96.35%\n","Best weighted precision - threshold [0.5,0.5] -> 90.42%\n","Best ALL recall         - threshold [0.5,0.5] -> 93.95%\n","Best weighted recall    - threshold [0.5,0.5] -> 90.49%\n","Best ALL f1-score       - threshold [0.5,0.5] -> 93.0%\n","Best weighted f1-score  - threshold [0.5,0.5] -> 90.44%\n","\n","Threshold used for adjusted predictions is: [ 0.5 , 0.5 ]\n","\n","Confusion matrix:\n","[[2857  184]\n"," [ 246 1235]]\n","              precision    recall  f1-score   support\n","\n","         all     0.9207    0.9395    0.9300      3041\n","         hem     0.8703    0.8339    0.8517      1481\n","\n","    accuracy                         0.9049      4522\n","   macro avg     0.8955    0.8867    0.8909      4522\n","weighted avg     0.9042    0.9049    0.9044      4522\n","\n","Change in accuracy           : 0.0 %\n","Change in ALL precision      : 0.0 %\n","Change in weighted precision : 0.0 %\n","Change in ALL recall         : 0.0 %\n","Change in weighted recall    : 0.0 %\n","Change in ALL F1-score       : 0.0 %\n","Change in weighted F1-score  : 0.0 %\n","\n","**********************************\n","Model # 2\n","**********************************\n","119/119 [==============================] - 22s 161ms/step - loss: 0.3723 - accuracy: 0.8901\n","--------------------\n","Original threshold\n","Test Loss:  0.3722901940345764\n","Test Accuracy:  0.8900928497314453\n","\n","Confusion matrix:\n","[[2945   96]\n"," [ 401 1080]]\n","              precision    recall  f1-score   support\n","\n","         all     0.8802    0.9684    0.9222      3041\n","         hem     0.9184    0.7292    0.8129      1481\n","\n","    accuracy                         0.8901      4522\n","   macro avg     0.8993    0.8488    0.8676      4522\n","weighted avg     0.8927    0.8901    0.8864      4522\n","\n","\n","For the threshold adjustments on test set predictions for Model 2\n","Best accuracy           - threshold [0.65,0.35] -> 90.18%\n","Best ALL precision      - threshold [0.9,0.1] -> 96.16%\n","Best weighted precision - threshold [0.65,0.35] -> 90.1%\n","Best ALL recall         - threshold [0.5,0.5] -> 96.84%\n","Best weighted recall    - threshold [0.65,0.35] -> 90.18%\n","Best ALL f1-score       - threshold [0.62,0.38] -> 92.85%\n","Best weighted f1-score  - threshold [0.67,0.33] -> 90.1%\n","\n","Threshold used for adjusted predictions is: [ 0.67 , 0.33 ]\n","\n","Confusion matrix:\n","[[2861  180]\n"," [ 264 1217]]\n","              precision    recall  f1-score   support\n","\n","         all     0.9155    0.9408    0.9280      3041\n","         hem     0.8712    0.8217    0.8457      1481\n","\n","    accuracy                         0.9018      4522\n","   macro avg     0.8933    0.8813    0.8869      4522\n","weighted avg     0.9010    0.9018    0.9010      4522\n","\n","Change in accuracy           : 1.172 %\n","Change in ALL precision      : 3.536 %\n","Change in weighted precision : 0.832 %\n","Change in ALL recall         : -2.762 %\n","Change in weighted recall    : 1.172 %\n","Change in ALL F1-score       : 0.581 %\n","Change in weighted F1-score  : 1.464 %\n","\n","**********************************\n","Model # 3\n","**********************************\n","119/119 [==============================] - 35s 225ms/step - loss: 0.3619 - accuracy: 0.8890\n","--------------------\n","Original threshold\n","Test Loss:  0.36194923520088196\n","Test Accuracy:  0.8889871835708618\n","\n","Confusion matrix:\n","[[2877  164]\n"," [ 338 1143]]\n","              precision    recall  f1-score   support\n","\n","         all     0.8949    0.9461    0.9198      3041\n","         hem     0.8745    0.7718    0.8199      1481\n","\n","    accuracy                         0.8890      4522\n","   macro avg     0.8847    0.8589    0.8698      4522\n","weighted avg     0.8882    0.8890    0.8871      4522\n","\n","\n","For the threshold adjustments on test set predictions for Model 3\n","Best accuracy           - threshold [0.51,0.49] -> 88.97%\n","Best ALL precision      - threshold [0.9,0.1] -> 94.26%\n","Best weighted precision - threshold [0.51,0.49] -> 88.88%\n","Best ALL recall         - threshold [0.5,0.5] -> 94.61%\n","Best weighted recall    - threshold [0.51,0.49] -> 88.97%\n","Best ALL f1-score       - threshold [0.51,0.49] -> 92.01%\n","Best weighted f1-score  - threshold [0.51,0.49] -> 88.79%\n","\n","Threshold used for adjusted predictions is: [ 0.51 , 0.49 ]\n","\n","Confusion matrix:\n","[[2873  168]\n"," [ 331 1150]]\n","              precision    recall  f1-score   support\n","\n","         all     0.8967    0.9448    0.9201      3041\n","         hem     0.8725    0.7765    0.8217      1481\n","\n","    accuracy                         0.8897      4522\n","   macro avg     0.8846    0.8606    0.8709      4522\n","weighted avg     0.8888    0.8897    0.8879      4522\n","\n","Change in accuracy           : 0.066 %\n","Change in ALL precision      : 0.182 %\n","Change in weighted precision : 0.058 %\n","Change in ALL recall         : -0.132 %\n","Change in weighted recall    : 0.066 %\n","Change in ALL F1-score       : 0.034 %\n","Change in weighted F1-score  : 0.081 %\n","\n","**********************************\n","Model # 4\n","**********************************\n","119/119 [==============================] - 28s 180ms/step - loss: 0.3548 - accuracy: 0.8735\n","--------------------\n","Original threshold\n","Test Loss:  0.3547992408275604\n","Test Accuracy:  0.8735073208808899\n","\n","Confusion matrix:\n","[[2926  115]\n"," [ 457 1024]]\n","              precision    recall  f1-score   support\n","\n","         all     0.8649    0.9622    0.9110      3041\n","         hem     0.8990    0.6914    0.7817      1481\n","\n","    accuracy                         0.8735      4522\n","   macro avg     0.8820    0.8268    0.8463      4522\n","weighted avg     0.8761    0.8735    0.8686      4522\n","\n","\n","For the threshold adjustments on test set predictions for Model 4\n","Best accuracy           - threshold [0.52,0.48] -> 87.53%\n","Best ALL precision      - threshold [0.9,0.1] -> 95.87%\n","Best weighted precision - threshold [0.52,0.48] -> 87.71%\n","Best ALL recall         - threshold [0.5,0.5] -> 96.22%\n","Best weighted recall    - threshold [0.52,0.48] -> 87.53%\n","Best ALL f1-score       - threshold [0.52,0.48] -> 91.19%\n","Best weighted f1-score  - threshold [0.64,0.36] -> 87.23%\n","\n","Threshold used for adjusted predictions is: [ 0.64 , 0.36 ]\n","\n","Confusion matrix:\n","[[2823  218]\n"," [ 352 1129]]\n","              precision    recall  f1-score   support\n","\n","         all     0.8891    0.9283    0.9083      3041\n","         hem     0.8382    0.7623    0.7984      1481\n","\n","    accuracy                         0.8739      4522\n","   macro avg     0.8636    0.8453    0.8534      4522\n","weighted avg     0.8724    0.8739    0.8723      4522\n","\n","Change in accuracy           : 0.044 %\n","Change in ALL precision      : 2.422 %\n","Change in weighted precision : -0.365 %\n","Change in ALL recall         : -3.387 %\n","Change in weighted recall    : 0.044 %\n","Change in ALL F1-score       : -0.266 %\n","Change in weighted F1-score  : 0.37 %\n","\n"]}],"source":["best_scores = []\n","y = 1\n","\n","for model in [model1, model2, model3, model4]:\n","  print('**********************************')\n","  print(f'Model # {y}')\n","  print('**********************************')\n","\n","  test_score = model.evaluate(test_gen, steps= test_steps, verbose= 1)\n","\n","  print('-' * 20)\n","  print('Original threshold')\n","  print(\"Test Loss: \", test_score[0])\n","  print(\"Test Accuracy: \", test_score[1])\n","\n","  # make predictions\n","  y_pred_probs = model.predict_generator(test_gen)\n","  y_pred_original = np.argmax(y_pred_probs, axis=1)\n","  cm = confusion_matrix(test_gen.classes, y_pred_original)\n","  print()\n","  print('Confusion matrix:')\n","  print(cm)\n","  print(classification_report(test_gen.classes, y_pred_original, target_names= classes, digits= 4))\n","\n","  # experimenting thresholds\n","  acc_list = []\n","  pre_list = []\n","  w_pre_list = []\n","  rec_list = []\n","  w_rec_list = []\n","  f1_list = []\n","  w_f1_list = []\n","\n","  thresholds = [0.5,0.5]\n","\n","  for x in range(81):\n","    thresholds[0] = 0.5 + x*0.005\n","    thresholds[1] = 0.5 - x*0.005\n","    y_pred_adjusted = (y_pred_probs >= thresholds).astype(int) # Applying the adjusted thresholds for each class\n","    y_pred_adjusted = np.argmax(y_pred_adjusted, axis=1)\n","\n","    acc_list.append(accuracy_score(test_gen.classes, y_pred_adjusted))\n","    pre_list.append( precision_score(test_gen.classes, y_pred_adjusted, pos_label= 0))\n","    w_pre_list.append( precision_score(test_gen.classes, y_pred_adjusted, average='weighted'))\n","    rec_list.append(recall_score(test_gen.classes, y_pred_adjusted, pos_label= 0))\n","    w_rec_list.append(recall_score(test_gen.classes, y_pred_adjusted, average='weighted'))\n","    f1_list.append(f1_score(test_gen.classes, y_pred_adjusted, pos_label= 0))\n","    w_f1_list.append(f1_score(test_gen.classes, y_pred_adjusted, average='weighted'))\n","\n","  # results\n","  best_acc_ndx = np.argmax(np.array(acc_list))\n","  best_pre_ndx = np.argmax(np.array(pre_list))\n","  best_w_pre_ndx = np.argmax(np.array(w_pre_list))\n","  best_rec_ndx = np.argmax(np.array(rec_list))\n","  best_w_rec_ndx = np.argmax(np.array(w_rec_list))\n","  best_f1_ndx = np.argmax(np.array(f1_list))\n","  best_w_f1_ndx = np.argmax(np.array(w_f1_list))\n","\n","  print()\n","  print(f'For the threshold adjustments on test set predictions for Model {y}')\n","  print(f'Best accuracy           - threshold [{round(0.5+0.005*best_acc_ndx,2)},{round(0.5-0.005*best_acc_ndx,2)}] -> {round(acc_list[best_acc_ndx]*100,2)}%')\n","  print(f'Best ALL precision      - threshold [{round(0.5+0.005*best_pre_ndx,2)},{round(0.5-0.005*best_pre_ndx,2)}] -> {round(pre_list[best_pre_ndx]*100,2)}%')\n","  print(f'Best weighted precision - threshold [{round(0.5+0.005*best_w_pre_ndx,2)},{round(0.5-0.005*best_w_pre_ndx,2)}] -> {round(w_pre_list[best_w_pre_ndx]*100,2)}%')\n","  print(f'Best ALL recall         - threshold [{round(0.5+0.005*best_rec_ndx,2)},{round(0.5-0.005*best_rec_ndx,2)}] -> {round(rec_list[best_rec_ndx]*100,2)}%')\n","  print(f'Best weighted recall    - threshold [{round(0.5+0.005*best_w_rec_ndx,2)},{round(0.5-0.005*best_w_rec_ndx,2)}] -> {round(w_rec_list[best_w_rec_ndx]*100,2)}%')\n","  print(f'Best ALL f1-score       - threshold [{round(0.5+0.005*best_f1_ndx,2)},{round(0.5-0.005*best_f1_ndx,2)}] -> {round(f1_list[best_f1_ndx]*100,2)}%')\n","  print(f'Best weighted f1-score  - threshold [{round(0.5+0.005*best_w_f1_ndx,2)},{round(0.5-0.005*best_w_f1_ndx,2)}] -> {round(w_f1_list[best_w_f1_ndx]*100,2)}%')\n","  print()\n","\n","  # Apply the chosen threshold\n","  best_ndx = best_w_f1_ndx # Here we use Weighted F1 score. Change here if you want a different criteria\n","\n","  thresholds = [round(0.5+0.005*best_ndx,2),round(0.5-0.005*best_ndx,2)]\n","  y_pred_adjusted = (y_pred_probs >= thresholds).astype(int) # Applying the adjusted thresholds for each class\n","  y_pred_adjusted = np.argmax(y_pred_adjusted, axis=1) # Getting the updated predictions\n","  cm = confusion_matrix(test_gen.classes, y_pred_adjusted)\n","\n","  #print adjusted metrics\n","  print(f'Threshold used for adjusted predictions is: [ {round(0.5+0.005*best_ndx,2)} , {round(0.5-0.005*best_ndx,2)} ]')\n","  print('')\n","  print('Confusion matrix:')\n","  print(cm)\n","  print(classification_report(test_gen.classes, y_pred_adjusted, target_names= classes, digits= 4))\n","\n","  print(f'Change in accuracy           : {round((acc_list[best_ndx]-acc_list[0])*100,3)} %')\n","  print(f'Change in ALL precision      : {round((pre_list[best_ndx]-pre_list[0])*100,3)} %')\n","  print(f'Change in weighted precision : {round((w_pre_list[best_ndx]-w_pre_list[0])*100,3)} %')\n","  print(f'Change in ALL recall         : {round((rec_list[best_ndx]-rec_list[0])*100,3)} %')\n","  print(f'Change in weighted recall    : {round((w_rec_list[best_ndx]-w_rec_list[0])*100,3)} %')\n","  print(f'Change in ALL F1-score       : {round((f1_list[best_ndx]-f1_list[0])*100,3)} %')\n","  print(f'Change in weighted F1-score  : {round((w_f1_list[best_ndx]-w_f1_list[0])*100,3)} %')\n","  print()\n","\n","  best_scores.append(w_f1_list[best_w_f1_ndx]*100) # change here according to criteria\n","  y += 1"]},{"cell_type":"markdown","metadata":{"id":"SsIK5v0lGG69"},"source":["#### **Score results**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zwnRlhGmJc08","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1692002945077,"user_tz":-480,"elapsed":23,"user":{"displayName":"Thesis TempF","userId":"15864590997921893499"}},"outputId":"b92ad2f1-876f-47f3-be2b-c268b55ccb0f"},"outputs":[{"output_type":"stream","name":"stdout","text":["Results (Weighted F1 Score comparison):\n","Model 1: 90.437 %\n","Model 2: 90.105 %\n","Model 3: 88.788 %\n","Model 4: 87.232 %\n","\n","Best result is Model 1 with score of 90.437 %\n"]}],"source":["print('Results (Weighted F1 Score comparison):')\n","for x in range(4):\n","  print(f'Model {x+1}: {round(best_scores[x],3)} %')\n","\n","print()\n","best_ndx = np.argmax(np.array(best_scores))\n","print(f'Best result is Model {best_ndx+1} with score of {round(best_scores[best_ndx],3)} %')"]}],"metadata":{"accelerator":"GPU","colab":{"machine_shape":"hm","provenance":[{"file_id":"1U7q-vAMdSvn2RmUJjk3aVwq_AfJVeMbP","timestamp":1686480148075},{"file_id":"1aDJV89PWP52Aak-w2uLGmx0uaJurrSC_","timestamp":1686478574396},{"file_id":"1Wf4CH2uw1nuboyPkdVP7Icg-ukKQFXBs","timestamp":1686473318602},{"file_id":"1oCDoN9WYO78kgsbfZ3imy4YBWqO9C28b","timestamp":1686472576968},{"file_id":"1eHI97040KJdTXlR5-p7_4bsxsa3oNR0Q","timestamp":1686472455797},{"file_id":"1ZUvlGeenrQ59o_5E9FSmztakUmV7NX9C","timestamp":1686470694174},{"file_id":"1qIEHE2MNSh3xa_lmK1BpRLKaOg9s-Q-r","timestamp":1686470290860},{"file_id":"1OutGxyMN3vGgEQ43nDDdF5lwM1vGYleG","timestamp":1686469745176},{"file_id":"1BO4pwKJs8e-1bPMgTR5n9EPKiQy_7_2r","timestamp":1686468445519},{"file_id":"1MnRhe2mr_uT_5WVwwaAgUTIdo65HxkZN","timestamp":1686467452477},{"file_id":"16OdRqX8c14JpOI3fkhJOQdW7BKqzxzvW","timestamp":1686466531944}],"gpuType":"T4"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.12"},"vscode":{"interpreter":{"hash":"4e8b91188996e708824b56d4504145ad3b01498257cb32dbe996bd862d0a9b3b"}}},"nbformat":4,"nbformat_minor":0}